<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks | Mscho's BLOG</title>
<meta name=keywords content="AI,Deep Learning,Generative models"><meta name=description content="Understand the main idea of StyleGAN, understand the new techniques they introduced, and examine its contributions to image generation"><meta name=author content="Minsung Cho"><link rel=canonical href=https://mscho2008.github.io/posts/stylegan/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://mscho2008.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://mscho2008.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://mscho2008.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://mscho2008.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://mscho2008.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://mscho2008.github.io/posts/stylegan/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks"><meta property="og:description" content="Understand the main idea of StyleGAN, understand the new techniques they introduced, and examine its contributions to image generation"><meta property="og:type" content="article"><meta property="og:url" content="https://mscho2008.github.io/posts/stylegan/"><meta property="og:image" content="https://mscho2008.github.io/%3Cimage%20path/url%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-01T00:00:00+00:00"><meta property="og:site_name" content="Mscho's BLOG"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://mscho2008.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks"><meta name=twitter:description content="Understand the main idea of StyleGAN, understand the new techniques they introduced, and examine its contributions to image generation"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://mscho2008.github.io/posts/"},{"@type":"ListItem","position":2,"name":"StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks","item":"https://mscho2008.github.io/posts/stylegan/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks","name":"StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks","description":"Understand the main idea of StyleGAN, understand the new techniques they introduced, and examine its contributions to image generation","keywords":["AI","Deep Learning","Generative models"],"articleBody":"Introduction The motivation for creating StyleGAN started from some issues with GANs. While GANs have a structure that can generate high-quality images, they face big challenges in controlling detailed styles or elements of the image. For example, in previous GAN models, even if they generate a person’s face, there were many limits in adjusting small details or styles of that face .\nStyleGAN aims to overcome these limits of regular GAN models and achieve both control over image generation and high-quality, detailed images. StyleGAN’s goal is to create images that are more realistic and controllable. To achieve this, StyleGAN’s design borrwed ideas from style transfer literature and added mechanisms to mix different styles.\nBasic Concepts of GANs and Limitations of Existing Models To better understand StyleGAN, this section will provide a brief explanation of its predecessors, GAN, Deep Convolutional GAN(DCGAN), and Progressive Growing GAN (PGGAN)\nGAN and Its Basic Structure GANs have a structure of two networks, the Generator and the Discriminator ,which compete and improve together. The Generator takes random noise as input to create fake images, while the Discriminator tries to distinguish between real and fake images. Through their Adverisal interaction in training, the Generator gradually improves and creates images that look more and more real.\nFig. 1. Architecture of a generative adversarial network. (Image source: www.kdnuggets.com/2017/01/generative-…-learning.html)\nFor details, these two models compete with each other in training process. G(generator) tries to trick the discriminator. Otherwise D(discriminator) works hard not to be fooled.\nIn other words, D and G are playing a minmax game to optimize the following loss function: Deep Convolutional GAN (DCGAN) DCGAN is a model that starts with a latent vector and progressively upsamples it through a convolutional network. This approach has improved performance in the image domain. By using transposed convolutions, DCGAN effectively increases the image resolution, step by step. With this structure, vector arithmetic became possible, leading to advancements in GANs that aimed to control semantic information within images.\nFig. 2. Architecture of a DCGAN. (Image source: https://www.researchgate.net/figure/DCGAN-Deep-Convolutional-Generative-Adversarial-Network-generator…..)\nProgressive Growing GAN (PGGAN) Progressive Growing GAN, the predecessor of StyleGAN, is a model that improves GAN training stability by sequentially adding layers to gradually increase the image resolution. Instead of creating high-resolution images all at once, this method ensures stability by incrementally adding layers during training.\nFig. 3. Architecture of a PGGAN. (Image source: [https://python.plainenglish.io/a-friendly-introduction-to-generative-adversarial-networks-gans-101f8de8d3b6])\nPGGAN applied WGAN-GP Loss to improve training stability, making it possible to generate high-resolution images. However, it had limitations in controlling specific details or styles within the images.\nMain Idea of StyleGAN These are brief sketches of the main idea.\nMapping Network: Instead of directly using the z-vector sampled from the Gaussian distribution, it is mapped to the w domain for use.\nConstant input : Start with 4x4x512 tensor rather than later vector\nAdaIN: Using AdaIN, which showed effective performance in feed-forward style transfer networks. We can extract and apply style information from other selected data.\nStochastic Variation: The model is designed to effectively incorporate stochastic variation during image generation, allowing for control over various probabilistic details.\nStyle Mixing : During training, a certain percentage of images are generated using two random latent codes instead of just one.\nMapping Network Fig. 4. Mapping Network for StyleGAN. (Image source: [ Karras, T., Laine, S., \u0026 Aila, T. (2019). “A Style-Based Generator Architecture for Generative Adversarial Networks”. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.])\nLet’s assume that the distribution of the training set is like (a) in Figure 4, where there is almost no data in the top left part of the distribution. Since z is sampled from a Gaussian distribution, it is simillar to sampling from a spherical shape. When we look at (b) in Figure 4, we can see that in the top left of the sphere, rapid changes in style can easily occur during the interpolation process. Also, we can observe that the factors of variation are not linear, and we refer to this as being entangled.\nIn StyleGAN, instead of using the z vector directly, it is mapped to the w space, where the w vector is then used. In the W space, the factors of variation become more linear ((c) in Figure4), because they no longer need to follow a specific distribution.\nFig. 5. Mapping Network for StyleGAN. (Image source: [ Karras, T., Laine, S., \u0026 Aila, T. (2019). “A Style-Based Generator Architecture for Generative Adversarial Networks”. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.])\nConstant input The style-based generator takes w as input, so unlike PGGAN or other GANs, it no longer requires convolution operations on z. As a result, the synthesis network starts with a 4x4x512 constant tensor. Starting from a constant rather than latent vector has proven to yield better performance, a result confirmed through empirical observation.\nAdaIN This method allows for adding multiple style details as the layers progress, creating more diverse images. Since style information is taken from other selected data, there are no parameters to train.\nIt is used to process the result of the convolution operation. Let’s assume that the convolution output is a tensor made up of n channels, and each channel feature is represented by x_i. The latent w undergoes an affine transformation to produce y_{s,i} and y_{b,i}. These values are used to scale and add bias to the normalized x_i. So the mean and variance are modified. This can also be seen as a type of style transfer.\nFig. 6. How AdaIN works in Style GAN. (Image source: https://velog.ioStyleGAN-A-Style-Based-Generator-Architecture-for-Generative-Adversarial-Networks…..)\nStochastic Variation Fig. 7. Total Architecture for StyleGAN. (Image source: [ Karras, T., Laine, S., \u0026 Aila, T. (2019). “A Style-Based Generator Architecture for Generative Adversarial Networks”. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.])\nEven when photographing the same person, variables like wind strength or the person’s condition can make probabilistic features, such as changes in hair placement or acne. To manage this type of stochastic variation, noise input undergoes an affine transformation, enabling it to influence the AdaIN layer immediately before application.\nTo make sure the difference between style and noise, Style influences high-level global attributes, like face shape or pose, while Noise, which controls stochastic variation, affects finer details like freckles and skin pores.\nStarting from a 4x4x512 tensor, a total of 9 blocks are needed to generate a high-resolution 1024x1024 image, with each block applying style twice. This results in a total of 18 mixed styles, where the first 4 are defined as Coarse style, the next 4 as Middle style, and the remaining 10 as Fine styles. The coarse style mainly determines large elements such as face shape, overall hairstyle, and pose. The middle style adjusts smaller facial features, such as whether the eyes are open or closed, and other finer facial details. Finally, the fine style controls subtle details like color composition and skin texture.\nFig. 8. Images were generated by copying a specified subset of styles from source B and taking the rest from source A. (Image source: [ Karras, T., Laine, S., \u0026 Aila, T. (2019). “A Style-Based Generator Architecture for Generative Adversarial Networks”. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.])\nStyle Mixing To make the style more localized, a certain percentage of images are generated during training using two random latent codes instead of just one. Specifically, two latent codes, z_1 and z_2 are passed through a mapping network to produce w_1 and w_2. When applying the style, the crossover point is chosen, and w_1 is applied before this point, while w_2 is applied after it. This regularization technique prevents the network from assuming that styles in neighboring layers are related, allowing each layer’s style to be more *Localized.\nFig. 9. FIDs in FFHQ for networks trained by enabling the mixing regularization for different percentage of training examples. (Image source: [ Karras, T., Laine, S., \u0026 Aila, T. (2019). “A Style-Based Generator Architecture for Generative Adversarial Networks”. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.])\nFFHQ DATASET Writers have introduced a new dataset of human faces called Flickr-Faces-HQ (FFHQ), which contains 70,000 high-quality images at a resolution of 1024x1024. This dataset shows much more variety than CELEBA-HQ in terms of age, ethnicity, and background. It also includes a wider range of accessories, such as eyeglasses, sunglasses, and hats.\nEvaluations Traditionally, FID (Fréchet Inception Distance) has been used to evaluate the performance of GAN networks. Performance was measured by adding each method to the baseline step-by-step, with the following results.\nFig. 10. Fr’echet inception distance (FID) for various generator designs. (Image source: [ Karras, T., Laine, S., \u0026 Aila, T. (2019). “A Style-Based Generator Architecture for Generative Adversarial Networks”. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.])\nDisentanglement Studies To quantitatively measure how well the styles are disentangled, two new methods have been proposed.\nPerceptual path length Fig. 11. Two different Methods of Interpolation. (Image source: [https://velog.io/@su1433/StyleGan])\nThere are two main methods for interpolating latent codes: LERP and SLERP. Perceptual Path Length measures how suddenly the features change when interpolating between two latent codes. In other words, it calculates the feature distance between points t and t+ϵ. This measurement uses a pre-trained VGG network.\nSince Z follows a Gaussian distribution, SLERP is used for its interpolation, On the other hand, W uses linear interpolation(LERP).\nFig. 12. Compute L_Z and l_W. (Image source: [https://velog.io/@su1433/StyleGan])\nLinear separability Linear separability evaluates how linearly separable the attributes are in latent space. CELEBA-HQ dataset (where each face is labeled with 40 binary attributes such as gender) is used for training. For each attribute, 200,000 images are generated and fed into a classification network. Then, the half with the lowest confidence is removed, leaving 100,000 latent vectors. A linear SVM model is used for each attribute to predict the attribute from both z and w.\nThe conditional entropy H(Y∣X)is then computed, where X are the classes predicted by the SVM, and Y are the classes determined by the pre-trained classifier. The final separability score is defined as exp(∑i H(Y_i∣X_i)) where i represents the 40 attributes.\nThe table shows that StyleGAN has lower separability, which implies that various features are well-separated, making them easier to control. Here, the number in the method shows the depth of the mapping network.\nFig. 13. The effect of a mapping network in FFHQ. (Image source: [https://velog.io/@su1433/StyleGan])\nSadly, StyleGAN has its own challenges. Even though it made big improvements in creating images, StyleGAN still has problems with training stability and computational efficiency. The model’s complex design, with its style-based layers and large mapping network, sometimes leads to long training times and high computing costs\nAlso, while StyleGAN tries to control specific image details well, it often overfits to certain styles during training, which can lead to decreased diversity in the images it generates. Separating style and content fully is also still hard, especially in unsupervised settings.\nSome improvements, like better disentanglement and reduced computing costs, were discussed in StyleGAN2 (Karras et al., 2020).\n","wordCount":"1832","inLanguage":"en","image":"https://mscho2008.github.io/%3Cimage%20path/url%3E","datePublished":"2024-11-01T00:00:00Z","dateModified":"2024-11-01T00:00:00Z","author":[{"@type":"Person","name":"Minsung Cho"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://mscho2008.github.io/posts/stylegan/"},"publisher":{"@type":"Organization","name":"Mscho's BLOG","logo":{"@type":"ImageObject","url":"https://mscho2008.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://mscho2008.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://mscho2008.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://mscho2008.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://mscho2008.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://mscho2008.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://mscho2008.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://sites.google.com/view/mscho2008/ title="about me"><span>about me</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://mscho2008.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://mscho2008.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks</h1><div class=post-description>Understand the main idea of StyleGAN, understand the new techniques they introduced, and examine its contributions to image generation</div><div class=post-meta><span title='2024-11-01 00:00:00 +0000 UTC'>November 1, 2024</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1832 words&nbsp;·&nbsp;Minsung Cho&nbsp;|&nbsp;<a href=https://arxiv.org/abs/1812.04948/posts/StyleGAN.md rel="noopener noreferrer" target=_blank>StyleGAN</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#basic-concepts-of-gans-and-limitations-of-existing-models>Basic Concepts of GANs and Limitations of Existing Models</a><ul><li><a href=#gan-and-its-basic-structure>GAN and Its Basic Structure</a></li><li><a href=#deep-convolutional-gan-dcgan>Deep Convolutional GAN (DCGAN)</a></li><li><a href=#progressive-growing-gan-pggan>Progressive Growing GAN (PGGAN)</a></li></ul></li><li><a href=#main-idea-of-stylegan>Main Idea of StyleGAN</a><ul><li><a href=#mapping-network>Mapping Network</a></li><li><a href=#constant-input>Constant input</a></li><li><a href=#adain>AdaIN</a></li><li><a href=#stochastic-variation>Stochastic Variation</a></li><li><a href=#style-mixing>Style Mixing</a></li></ul></li><li><a href=#ffhq-dataset>FFHQ DATASET</a></li><li><a href=#evaluations>Evaluations</a></li><li><a href=#disentanglement-studies>Disentanglement Studies</a><ul><li><a href=#perceptual-path-length>Perceptual path length</a></li><li><a href=#linear-separability>Linear separability</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>The motivation for creating StyleGAN started from some issues with GANs. While GANs have a structure that can generate high-quality images, they face big challenges in controlling detailed styles or elements of the image. For example, in previous GAN models, even if they generate a person’s face, there were many limits in adjusting small details or styles of that face .</p><p>StyleGAN aims to overcome these limits of regular GAN models and achieve both control over image generation and high-quality, detailed images. StyleGAN’s goal is to create images that are more realistic and controllable. To achieve this, StyleGAN&rsquo;s design borrwed ideas from style transfer literature and added mechanisms to mix different styles.</p><h2 id=basic-concepts-of-gans-and-limitations-of-existing-models>Basic Concepts of GANs and Limitations of Existing Models<a hidden class=anchor aria-hidden=true href=#basic-concepts-of-gans-and-limitations-of-existing-models>#</a></h2><p>To better understand StyleGAN, this section will provide a brief explanation of its predecessors, GAN, Deep Convolutional GAN(DCGAN), and Progressive Growing GAN (PGGAN)</p><h3 id=gan-and-its-basic-structure>GAN and Its Basic Structure<a hidden class=anchor aria-hidden=true href=#gan-and-its-basic-structure>#</a></h3><p>GANs have a structure of two networks, the Generator and the Discriminator ,which compete and improve together. The Generator takes random noise as input to create fake images, while the Discriminator tries to distinguish between real and fake images. Through their Adverisal interaction in training, the Generator gradually improves and creates images that look more and more real.</p><p><img loading=lazy src=/image/stylegan/ganstructure.png alt="GAN structure">
<em>Fig. 1. Architecture of a generative adversarial network. (Image source: <a href=https://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html>www.kdnuggets.com/2017/01/generative-&mldr;-learning.html</a>)</em></p><p>For details, these two models compete with each other in training process. G(generator) tries to trick the discriminator. Otherwise D(discriminator) works hard not to be fooled.</p><p>In other words, D and G are playing a <strong>minmax</strong> game to optimize the following loss function:
<img loading=lazy src=/image/stylegan/minmaxgame.png alt="Loss function"></p><h3 id=deep-convolutional-gan-dcgan>Deep Convolutional GAN (DCGAN)<a hidden class=anchor aria-hidden=true href=#deep-convolutional-gan-dcgan>#</a></h3><p>DCGAN is a model that starts with a latent vector and progressively upsamples it through a convolutional network. This approach has improved performance in the image domain. By using transposed convolutions, DCGAN effectively increases the image resolution, step by step. With this structure, vector arithmetic became possible, leading to advancements in GANs that aimed to control semantic information within images.</p><p><img loading=lazy src=/image/stylegan/dcganstructure.png alt="DCGAN structure">
<em>Fig. 2. Architecture of a DCGAN. (Image source: <a href=https://www.researchgate.net/figure/DCGAN-Deep-Convolutional-Generative-Adversarial-Network-generator-used-for-LSUN_fig1_340884113>https://www.researchgate.net/figure/DCGAN-Deep-Convolutional-Generative-Adversarial-Network-generator&mldr;..</a>)</em></p><h3 id=progressive-growing-gan-pggan>Progressive Growing GAN (PGGAN)<a hidden class=anchor aria-hidden=true href=#progressive-growing-gan-pggan>#</a></h3><p>Progressive Growing GAN, the predecessor of StyleGAN, is a model that improves GAN training stability by sequentially adding layers to gradually increase the image resolution. Instead of creating high-resolution images all at once, this method ensures stability by incrementally adding layers during training.</p><p><img loading=lazy src=/image/stylegan/pggan.png alt="PGGAN structure">
<em>Fig. 3. Architecture of a PGGAN. (Image source: [https://python.plainenglish.io/a-friendly-introduction-to-generative-adversarial-networks-gans-101f8de8d3b6]</em>)</p><p>PGGAN applied WGAN-GP Loss to improve training stability, making it possible to generate high-resolution images. However, it had limitations in controlling specific details or styles within the images.</p><h2 id=main-idea-of-stylegan>Main Idea of StyleGAN<a hidden class=anchor aria-hidden=true href=#main-idea-of-stylegan>#</a></h2><p>These are brief sketches of the main idea.</p><ul><li><p><strong>Mapping Network</strong>: Instead of directly using the z-vector sampled from the Gaussian distribution, it is mapped to the w domain for use.</p></li><li><p><strong>Constant input</strong> : Start with 4x4x512 tensor rather than later vector</p></li><li><p><strong>AdaIN</strong>: Using AdaIN, which showed effective performance in feed-forward style transfer networks. We can extract and apply style information from other selected data.</p></li><li><p><strong>Stochastic Variation</strong>: The model is designed to effectively incorporate stochastic variation during image generation, allowing for control over various probabilistic details.</p></li><li><p><strong>Style Mixing</strong> : During training, a certain percentage of images are generated using two random latent codes instead of just one.</p></li></ul><h3 id=mapping-network>Mapping Network<a hidden class=anchor aria-hidden=true href=#mapping-network>#</a></h3><p><img loading=lazy src=/image/stylegan/mappingnetwork.png#center alt="Mapping Network">
<em>Fig. 4. Mapping Network for StyleGAN. (Image source: [ Karras, T., Laine, S., & Aila, T. (2019). &ldquo;A Style-Based Generator Architecture for Generative Adversarial Networks&rdquo;. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.]</em>)</p><p>Let’s assume that the distribution of the training set is like (a) in Figure 4, where there is almost no data in the top left part of the distribution. Since z is sampled from a Gaussian distribution, it is simillar to sampling from a spherical shape. When we look at (b) in Figure 4,
we can see that in the top left of the sphere, <strong>rapid changes in style can easily occur</strong> during the interpolation process. Also, we can observe that the factors of variation are not linear, and we refer to this as being <strong>entangled</strong>.</p><p>In StyleGAN, instead of using the z vector directly, it is mapped to the w space, where the w vector is then used. In the W space, the factors of variation become more linear ((c) in Figure4), because they no longer need to follow a specific distribution.</p><p><img loading=lazy src=/image/stylegan/mappingnetwork2.png#center alt="Mapping Network">
<em>Fig. 5. Mapping Network for StyleGAN. (Image source: [ Karras, T., Laine, S., & Aila, T. (2019). &ldquo;A Style-Based Generator Architecture for Generative Adversarial Networks&rdquo;. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.]</em>)</p><h3 id=constant-input>Constant input<a hidden class=anchor aria-hidden=true href=#constant-input>#</a></h3><p>The style-based generator takes w as input, so unlike PGGAN or other GANs, it no longer requires convolution operations on z. As a result, the <strong>synthesis network starts with a 4x4x512 constant tensor</strong>. Starting from a constant rather than latent vector has proven to yield better performance, a result confirmed through empirical observation.</p><h3 id=adain>AdaIN<a hidden class=anchor aria-hidden=true href=#adain>#</a></h3><p>This method allows for adding multiple style details as the layers progress, creating more diverse images. Since style information is taken from other selected data, there are <strong>no parameters to train</strong>.</p><p>It is used to process the result of the convolution operation. Let’s assume that the convolution output is a tensor made up of n channels, and each channel feature is represented by x_i. The latent w undergoes an affine transformation to produce y_{s,i} and y_{b,i}. These values are used to <strong>scale and add bias</strong> to the normalized x_i. So the mean and variance are modified. This can also be seen as a type of style transfer.</p><p><img loading=lazy src=/image/stylegan/adain.png#center alt=AdaIN>
<em>Fig. 6. How AdaIN works in Style GAN. (Image source: <a href=https://velog.io/@minjung-s/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0StyleGAN-A-Style-Based-Generator-Architecture-for-Generative-Adversarial-Networks>https://velog.ioStyleGAN-A-Style-Based-Generator-Architecture-for-Generative-Adversarial-Networks&mldr;..</a>)</em></p><h3 id=stochastic-variation>Stochastic Variation<a hidden class=anchor aria-hidden=true href=#stochastic-variation>#</a></h3><p><img loading=lazy src=/image/stylegan/noise.png#center alt="Stochastic Variation">
<em>Fig. 7. Total Architecture for StyleGAN. (Image source: [ Karras, T., Laine, S., & Aila, T. (2019). &ldquo;A Style-Based Generator Architecture for Generative Adversarial Networks&rdquo;. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.]</em>)</p><p>Even when photographing the same person, variables like wind strength or the person&rsquo;s condition can make probabilistic features, such as changes in hair placement or acne. To manage this type of stochastic variation, noise input undergoes an affine transformation, enabling it to influence the AdaIN layer immediately before application.</p><p>To make sure the difference between style and noise, Style influences high-level global attributes, like face shape or pose, while Noise, which controls stochastic variation, affects finer details like freckles and skin pores.</p><p>Starting from a 4x4x512 tensor, a total of 9 blocks are needed to generate a high-resolution 1024x1024 image, with each block applying style twice. This results in a total of 18 mixed styles, where the first 4 are defined as <strong>Coarse style</strong>, the next 4 as <strong>Middle style</strong>, and the remaining 10 as <strong>Fine styles</strong>. The coarse style mainly determines large elements such as face shape, overall hairstyle, and pose. The middle style adjusts smaller facial features, such as whether the eyes are open or closed, and other finer facial details. Finally, the fine style controls subtle details like color composition and skin texture.</p><p><img loading=lazy src=/image/stylegan/styles.png#center alt=Styles>
<em>Fig. 8. Images were generated by copying a specified subset of styles from source B and taking the rest from source A. (Image source: [ Karras, T., Laine, S., & Aila, T. (2019). &ldquo;A Style-Based Generator Architecture for Generative Adversarial Networks&rdquo;. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.]</em>)</p><h3 id=style-mixing>Style Mixing<a hidden class=anchor aria-hidden=true href=#style-mixing>#</a></h3><p>To make the style more localized, a certain percentage of images are generated during training using two random latent codes instead of just one. Specifically, two latent codes, z_1 and z_2 are passed through a mapping network to produce
w_1 and w_2. When applying the style, the crossover point is chosen, and w_1 is applied before this point, while w_2 is applied after it. This regularization technique prevents the network from assuming that styles in neighboring layers are related, allowing each layer&rsquo;s style to be more *<strong>Localized</strong>.</p><p><img loading=lazy src=/image/stylegan/mixing.png#center alt=Styles>
<em>Fig. 9. FIDs in FFHQ for networks trained by enabling the mixing regularization for different percentage of training examples. (Image source: [ Karras, T., Laine, S., & Aila, T. (2019). &ldquo;A Style-Based Generator Architecture for Generative Adversarial Networks&rdquo;. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.]</em>)</p><h2 id=ffhq-dataset>FFHQ DATASET<a hidden class=anchor aria-hidden=true href=#ffhq-dataset>#</a></h2><p>Writers have introduced a new dataset of human faces called Flickr-Faces-HQ (FFHQ), which contains 70,000 high-quality images at a resolution of 1024x1024. This dataset shows much more variety than CELEBA-HQ in terms of age, ethnicity, and background. It also includes a wider range of accessories, such as eyeglasses, sunglasses, and hats.</p><h2 id=evaluations>Evaluations<a hidden class=anchor aria-hidden=true href=#evaluations>#</a></h2><p>Traditionally, FID (Fréchet Inception Distance) has been used to evaluate the performance of GAN networks. Performance was measured by adding each method to the baseline step-by-step, with the following results.</p><p><img loading=lazy src=/image/stylegan/evaluation.png#center alt=Styles>
<em>Fig. 10. Fr&rsquo;echet inception distance (FID) for various generator designs. (Image source: [ Karras, T., Laine, S., & Aila, T. (2019). &ldquo;A Style-Based Generator Architecture for Generative Adversarial Networks&rdquo;. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410.]</em>)</p><h2 id=disentanglement-studies>Disentanglement Studies<a hidden class=anchor aria-hidden=true href=#disentanglement-studies>#</a></h2><p>To <strong>quantitatively</strong> measure how well the styles are disentangled, two new methods have been proposed.</p><h3 id=perceptual-path-length>Perceptual path length<a hidden class=anchor aria-hidden=true href=#perceptual-path-length>#</a></h3><p><img loading=lazy src=/image/stylegan/interpolation.png alt="Interpolation method">
<em>Fig. 11. Two different Methods of Interpolation. (Image source: [https://velog.io/@su1433/StyleGan]</em>)</p><p>There are two main methods for interpolating latent codes: LERP and SLERP. Perceptual Path Length measures how suddenly the features change when interpolating between two latent codes. In other words, it calculates the feature distance between points t and t+ϵ. This measurement uses a pre-trained VGG network.</p><p>Since Z follows a Gaussian distribution, SLERP is used for its interpolation, On the other hand, W uses linear interpolation(LERP).</p><p><img loading=lazy src=/image/stylegan/lz.png#center alt=L_Z>
<img loading=lazy src=/image/stylegan/lw.png#center alt=L_W>
<em>Fig. 12. Compute L_Z and l_W. (Image source: [https://velog.io/@su1433/StyleGan]</em>)</p><h3 id=linear-separability>Linear separability<a hidden class=anchor aria-hidden=true href=#linear-separability>#</a></h3><p>Linear separability evaluates how linearly separable the attributes are in latent space. CELEBA-HQ dataset (where each face is labeled with 40 binary attributes such as gender) is used for training. For each attribute, 200,000 images are generated and fed into a classification network. Then, the half with the lowest confidence is removed, leaving 100,000 latent vectors. A linear SVM model is used for each attribute to predict the attribute from both z and w.</p><p>The conditional entropy H(Y∣X)is then computed, where X are the classes predicted by the SVM, and Y are the classes determined by the pre-trained classifier. The final separability score is defined as <strong>exp(∑i H(Y_i∣X_i))</strong> where i represents the 40 attributes.</p><p>The table shows that StyleGAN has lower separability, which implies that <strong>various features are well-separated, making them easier to control</strong>. Here, the number in the method shows the depth of the mapping network.</p><p><img loading=lazy src=/image/stylegan/eval2.png#center alt=eval2>
<em>Fig. 13. The effect of a mapping network in FFHQ. (Image source: [https://velog.io/@su1433/StyleGan]</em>)</p><hr><p>Sadly, StyleGAN has its own challenges. Even though it made big improvements in creating images, StyleGAN still has problems with training stability and computational efficiency. The model’s complex design, with its style-based layers and large mapping network, sometimes leads to long training times and high computing costs</p><p>Also, while StyleGAN tries to control specific image details well, it often overfits to certain styles during training, which can lead to decreased diversity in the images it generates. Separating style and content fully is also still hard, especially in unsupervised settings.</p><p>Some improvements, like better disentanglement and reduced computing costs, were discussed in StyleGAN2 (Karras et al., 2020).</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://mscho2008.github.io/tags/ai/>AI</a></li><li><a href=https://mscho2008.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://mscho2008.github.io/tags/generative-models/>Generative Models</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks on x" href="https://x.com/intent/tweet/?text=StyleGAN%20%3a%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks&amp;url=https%3a%2f%2fmscho2008.github.io%2fposts%2fstylegan%2f&amp;hashtags=AI%2cDeepLearning%2cGenerativemodels"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmscho2008.github.io%2fposts%2fstylegan%2f&amp;title=StyleGAN%20%3a%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks&amp;summary=StyleGAN%20%3a%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks&amp;source=https%3a%2f%2fmscho2008.github.io%2fposts%2fstylegan%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fmscho2008.github.io%2fposts%2fstylegan%2f&title=StyleGAN%20%3a%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmscho2008.github.io%2fposts%2fstylegan%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks on whatsapp" href="https://api.whatsapp.com/send?text=StyleGAN%20%3a%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks%20-%20https%3a%2f%2fmscho2008.github.io%2fposts%2fstylegan%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks on telegram" href="https://telegram.me/share/url?text=StyleGAN%20%3a%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks&amp;url=https%3a%2f%2fmscho2008.github.io%2fposts%2fstylegan%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks on ycombinator" href="https://news.ycombinator.com/submitlink?t=StyleGAN%20%3a%20A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks&u=https%3a%2f%2fmscho2008.github.io%2fposts%2fstylegan%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://mscho2008.github.io/>Mscho's BLOG</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>